{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50b4bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d175dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1633 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_SIZE = 32\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(os.path.join(os.getcwd(), 'entrenamiento'),seed =123,image_size=(IMG_SIZE,IMG_SIZE))\n",
    "classes_train = train_ds.class_names\n",
    "nClasses_train = len(classes_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9722a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer imagenes\n",
    "def read_images(dirname):\n",
    "    imgpath = dirname + os.sep\n",
    "    images = []\n",
    "    directories = []\n",
    "    dircount = []\n",
    "    prevRoot=''\n",
    "    cant=0\n",
    "    print(\"leyendo imagenes de \",imgpath)\n",
    "\n",
    "    for root, dirnames, filenames in os.walk(imgpath):\n",
    "        for filename in filenames:\n",
    "            if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
    "                cant+=1\n",
    "                filepath = os.path.join(root, filename)\n",
    "                # image = plt.imread(filepath)\n",
    "                image = cv2.imread(filepath)\n",
    "                plt.imshow(image)\n",
    "                image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "                images.append(image)\n",
    "                if prevRoot !=root:\n",
    "                    prevRoot=root\n",
    "                    directories.append(root)\n",
    "                    dircount.append(cant)\n",
    "                    cant=0\n",
    "    dircount.append(cant)\n",
    "\n",
    "    dircount = dircount[1:]\n",
    "    dircount[0]=dircount[0]+1\n",
    "    print('Directorios leidos:',len(directories))\n",
    "    print(\"Imagenes en cada directorio\", dircount)\n",
    "    print('Suma Total de imagenes en subdirs:',sum(dircount))\n",
    "    \n",
    "    tipos=[]\n",
    "    indice=0\n",
    "    for directorio in directories:\n",
    "        name = directorio.split(os.sep)\n",
    "        print(indice , name[len(name)-1])\n",
    "        tipos.append(name[len(name)-1])\n",
    "        indice=indice+1\n",
    "\n",
    "    labels=[]\n",
    "    indice=0\n",
    "    for cantidad in dircount:\n",
    "        for i in range(cantidad):\n",
    "            labels.append(tipos[indice])\n",
    "        indice=indice+1\n",
    "\n",
    "    X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
    "    y = np.array(labels)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9dc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leyendo imagenes de  C:\\Users\\juana\\Downloads\\practica3\\entrenamiento\\\n",
      "Directorios leidos: 7\n",
      "Imagenes en cada directorio [63, 213, 105, 949, 37, 204, 62]\n",
      "Suma Total de imagenes en subdirs: 1633\n",
      "0 CLASS_02\n",
      "1 CLASS_03\n",
      "2 CLASS_04\n",
      "3 CLASS_05\n",
      "4 CLASS_06\n",
      "5 CLASS_07\n",
      "6 CLASS_08\n",
      "leyendo imagenes de  C:\\Users\\juana\\Downloads\\practica3\\test\\\n",
      "Directorios leidos: 8\n",
      "Imagenes en cada directorio [2, 48, 97, 45, 459, 19, 114, 26]\n",
      "Suma Total de imagenes en subdirs: 810\n",
      "0 CLASS_01\n",
      "1 CLASS_02\n",
      "2 CLASS_03\n",
      "3 CLASS_04\n",
      "4 CLASS_05\n",
      "5 CLASS_06\n",
      "6 CLASS_07\n",
      "7 CLASS_08\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train = read_images(os.path.join(os.getcwd(), 'entrenamiento'))\n",
    "X_test,y_test = read_images(os.path.join(os.getcwd(), 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d278f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch_size = 75\n",
    "\n",
    "classifier_model = Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255,input_shape = (IMG_SIZE,IMG_SIZE,3)),\n",
    "    Conv2D(16,3,activation='relu',padding='same'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32,3,activation='relu',padding='same'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64,3,activation='relu',padding='same'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128,activation='relu'),\n",
    "    Dense(nClasses_train)\n",
    "])\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "classifier_model.compile(optimizer=optimizer, \n",
    "                        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                        metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dcd314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "52/52 [==============================] - 2s 17ms/step - loss: 1.3774 - accuracy: 0.5732\n",
      "Epoch 2/40\n",
      "52/52 [==============================] - 1s 18ms/step - loss: 1.0379 - accuracy: 0.6546\n",
      "Epoch 3/40\n",
      "52/52 [==============================] - 1s 18ms/step - loss: 0.7666 - accuracy: 0.7189\n",
      "Epoch 4/40\n",
      "52/52 [==============================] - 1s 19ms/step - loss: 0.6085 - accuracy: 0.7740\n",
      "Epoch 5/40\n",
      "52/52 [==============================] - 1s 18ms/step - loss: 0.5692 - accuracy: 0.7844\n",
      "Epoch 6/40\n",
      "52/52 [==============================] - 1s 18ms/step - loss: 0.5334 - accuracy: 0.7955\n",
      "Epoch 7/40\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.4408 - accuracy: 0.8328\n",
      "Epoch 8/40\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.3821 - accuracy: 0.8506\n",
      "Epoch 9/40\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.3937 - accuracy: 0.8512\n",
      "Epoch 10/40\n",
      "52/52 [==============================] - 1s 17ms/step - loss: 0.3716 - accuracy: 0.8555\n",
      "Epoch 11/40\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.3359 - accuracy: 0.8726\n",
      "Epoch 12/40\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.3021 - accuracy: 0.8892\n",
      "Epoch 13/40\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.2965 - accuracy: 0.8873\n",
      "Epoch 14/40\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.3015 - accuracy: 0.8861\n",
      "Epoch 15/40\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.2995 - accuracy: 0.8953\n",
      "Epoch 16/40\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.2389 - accuracy: 0.9167\n",
      "Epoch 17/40\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.2367 - accuracy: 0.9088\n",
      "Epoch 18/40\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.2334 - accuracy: 0.9143\n",
      "Epoch 19/40\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.1908 - accuracy: 0.9296\n",
      "Epoch 20/40\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.1897 - accuracy: 0.9314\n",
      "Epoch 21/40\n",
      "52/52 [==============================] - 1s 17ms/step - loss: 0.2091 - accuracy: 0.9247\n",
      "Epoch 22/40\n",
      "52/52 [==============================] - 1s 18ms/step - loss: 0.2309 - accuracy: 0.9063\n",
      "Epoch 23/40\n",
      "52/52 [==============================] - 1s 17ms/step - loss: 0.1857 - accuracy: 0.9357\n",
      "Epoch 24/40\n",
      "52/52 [==============================] - 1s 17ms/step - loss: 0.1833 - accuracy: 0.9363\n",
      "Epoch 25/40\n",
      "52/52 [==============================] - 1s 18ms/step - loss: 0.1675 - accuracy: 0.9430\n",
      "Epoch 26/40\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.1409 - accuracy: 0.9522\n",
      "Epoch 27/40\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.6690 - accuracy: 0.7606\n",
      "Epoch 28/40\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.2544 - accuracy: 0.9075\n",
      "Epoch 29/40\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.1732 - accuracy: 0.9363\n",
      "Epoch 30/40\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.2127 - accuracy: 0.9222\n",
      "Epoch 31/40\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.1581 - accuracy: 0.9467\n",
      "Epoch 32/40\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.4729 - accuracy: 0.8432\n",
      "Epoch 33/40\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.1548 - accuracy: 0.9547\n",
      "Epoch 34/40\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.1339 - accuracy: 0.9547\n",
      "Epoch 35/40\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.1213 - accuracy: 0.9559\n",
      "Epoch 36/40\n",
      "52/52 [==============================] - 1s 13ms/step - loss: 0.1144 - accuracy: 0.9571\n",
      "Epoch 37/40\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.1399 - accuracy: 0.9492\n",
      "Epoch 38/40\n",
      "52/52 [==============================] - 1s 16ms/step - loss: 0.1131 - accuracy: 0.9584\n",
      "Epoch 39/40\n",
      "52/52 [==============================] - 1s 19ms/step - loss: 0.0840 - accuracy: 0.9712\n",
      "Epoch 40/40\n",
      "52/52 [==============================] - 1s 15ms/step - loss: 0.0977 - accuracy: 0.9639\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier_train = classifier_model.fit(train_ds,batch_size=batch_size,epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43c619fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 810 images belonging to 8 classes.\n",
      "26/26 [==============================] - 3s 102ms/step\n",
      "Valores de test correctos 722 de 810\n",
      "Porcentaje correcto 89.14\n",
      "Matriz de confusion \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   1,   0],\n",
       "       [  0,  36,   0,   9,   0,   0,   3,   0],\n",
       "       [  0,   0,  74,   0,   1,   0,  22,   0],\n",
       "       [  0,   2,   1,  41,   0,   0,   1,   0],\n",
       "       [  0,   1,   9,   0, 439,   0,   5,   5],\n",
       "       [  0,   0,   0,   0,   4,  15,   0,   0],\n",
       "       [  0,   0,  10,   2,   1,   0, 101,   0],\n",
       "       [  0,   0,   1,   0,  10,   0,   0,  16]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator()\n",
    "test_generator = datagen.flow_from_directory(os.path.join(os.getcwd(), 'test'),\n",
    "                                            target_size = (IMG_SIZE,IMG_SIZE),class_mode = 'categorical',shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "predicted_test = classifier_model.predict(test_generator)\n",
    "\n",
    "y_predict_test = np.argmax(predicted_test,axis=1)\n",
    "y_real_test = test_generator.classes\n",
    "correct_test = np.where(y_predict_test==y_real_test)[0]\n",
    "\n",
    "\n",
    "print ('Valores de test correctos %s de %s'%(len(correct_test),len(y_real_test)))\n",
    "print ('Porcentaje correcto %.2f'%(len(correct_test)/len(y_real_test)*100))\n",
    "print ('Matriz de confusion ')\n",
    "confusion_matrix(y_real_test, y_predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd5faf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1633 images belonging to 8 classes.\n",
      "52/52 [==============================] - 6s 105ms/step\n",
      "Valores de test correctos 97.98\n",
      "Porcentaje correcto 1600 from 1633\n",
      "Matriz de confusion \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 59,   0,   3,   0,   0,   0,   0],\n",
       "       [  0, 200,   0,   0,   0,  13,   0],\n",
       "       [  0,   0, 105,   0,   0,   0,   0],\n",
       "       [  0,   7,   0, 938,   0,   3,   1],\n",
       "       [  0,   0,   0,   1,  36,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 204,   0],\n",
       "       [  0,   0,   0,   5,   0,   0,  58]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(os.path.join(os.getcwd(), 'entrenamiento'),\n",
    "                                            target_size = (IMG_SIZE,IMG_SIZE),class_mode = 'categorical',shuffle=False)\n",
    "\n",
    "predicted_train = classifier_model.predict(train_generator)\n",
    "y_predict_train = np.argmax(predicted_train,axis=1)\n",
    "y_real_train = train_generator.classes\n",
    "correct_train = np.where(y_predict_train==y_real_train)[0]\n",
    "\n",
    "print ('Valores de test correctos %.2f'%(len(correct_train)/len(y_real_train)*100))\n",
    "print ('Porcentaje correcto %s from %s'%(len(correct_train),len(y_real_train)))\n",
    "print ('Matriz de confusion ')\n",
    "confusion_matrix(y_real_train, y_predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056288f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
